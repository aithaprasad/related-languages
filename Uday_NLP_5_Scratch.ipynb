{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNk0623BW8gHpfl/vXI3wH6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aithaprasad/related-languages/blob/main/Uday_NLP_5_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We just read the file and return sentences based on the given start and end tags"
      ],
      "metadata": {
        "id": "iXZsKewruNQ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pn-BvGdpfpm3"
      },
      "outputs": [],
      "source": [
        "def get_sentences_from_file(file):\n",
        "  sentences = []\n",
        "  temp = []\n",
        "  data = open(file, 'r')\n",
        "  for line in data:\n",
        "    token = line.rstrip(\"\\n\")\n",
        "    if token == '<s>': temp = []\n",
        "    elif token == '</s>': sentences.append(temp)\n",
        "    else: temp.append(line.split('\\n')[0])\n",
        "  return sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_source = get_sentences_from_file('train-source.txt')\n",
        "train_target = get_sentences_from_file('train-target.txt')"
      ],
      "metadata": {
        "id": "moJtVMhOhSxt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_source = get_sentences_from_file('test-source.txt')\n",
        "test_target = get_sentences_from_file('test-target.txt')"
      ],
      "metadata": {
        "id": "i2xLNYy0rHLP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_source) == len(train_target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3vz6kqWhy0h",
        "outputId": "48cc6f96-8232-4c42-b445-1d33960da744"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_source, val_source = train_source[:-5000], train_source[-5000:]\n",
        "train_target, val_target = train_target[:-5000], train_target[-5000:]"
      ],
      "metadata": {
        "id": "0luDsPi-d1h2"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function returns transmission probabilities indicating with what probablity does a particular word get turn into a new word"
      ],
      "metadata": {
        "id": "2_3dG5H4yos4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def source_to_target_words_map(source, target):\n",
        "  \n",
        "  transmission_counts, transmission_probs = dict(), dict()\n",
        "  \n",
        "  for i in range(len(source)):\n",
        "    for j in range(min(len(source[i]), len(target[i]))):\n",
        "      if source[i][j] not in transmission_counts:\n",
        "        transmission_counts[source[i][j]] = {target[i][j] : 1}\n",
        "      elif target[i][j] not in transmission_counts[source[i][j]]:\n",
        "        transmission_counts[source[i][j]][target[i][j]] = 1\n",
        "      else:\n",
        "        transmission_counts[source[i][j]][target[i][j]] += 1\n",
        "\n",
        "  for key, data_dict in transmission_counts.items():\n",
        "      transmission_probs[key] = {}\n",
        "      dict_sum = sum(list(data_dict.values()))\n",
        "\n",
        "      for new_key, value in data_dict.items():\n",
        "          transmission_probs[key][new_key] = (transmission_counts[key][new_key] / dict_sum)\n",
        "\n",
        "    \n",
        "  return transmission_probs"
      ],
      "metadata": {
        "id": "rVdgdRC8l27j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transmission_probabilities = source_to_target_words_map(train_source, train_target)"
      ],
      "metadata": {
        "id": "dgl5yScfhnBg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This predict is almost similar to the viterbi function from my Kreyol segmentation assignment."
      ],
      "metadata": {
        "id": "9lSnnmVBy82w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(test):\n",
        "    all_sentences = []\n",
        "    for sentence in test:\n",
        "        curr_sentence = []\n",
        "        for word in sentence:\n",
        "            if word in transmission_probabilities:\n",
        "                curr_high_probability, translated_word = 0, ''\n",
        "                for key in transmission_probabilities[word]:\n",
        "                    if transmission_probabilities[word][key] > curr_high_probability:\n",
        "                        curr_high_probability = transmission_probabilities[word][key]\n",
        "                        translated_word = key\n",
        "                curr_sentence.append(translated_word)\n",
        "        all_sentences.append(curr_sentence)\n",
        "    return all_sentences"
      ],
      "metadata": {
        "id": "2FKtVLKqlnHr"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "metadata": {
        "id": "y727_--o1uXY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test_sentences = predict(test_source)"
      ],
      "metadata": {
        "id": "zaZfJKdJq5NE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_target_for_bleu = []\n",
        "for sentence in test_target:\n",
        "    test_target_for_bleu.append([sentence])"
      ],
      "metadata": {
        "id": "WQQdx3465027"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_bleu(test_target_for_bleu, predicted_test_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jlLvKudirjGH",
        "outputId": "58fb9aec-642c-402f-ee02-8e157f7d0538"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7421058136552283"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uhp8KvjH2gbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}