{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from pickle import load\n",
        "import tensorflow as tf\n",
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import RepeatVector\n",
        "from tensorflow.keras.layers import TimeDistributed\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import re"
      ],
      "metadata": {
        "id": "pDGEGpcHVMjm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(file):\n",
        "  data = []\n",
        "  with open(file,'r',encoding='utf-8') as f:\n",
        "    lines = f.read()\n",
        "    lines = lines.strip().split('</s>')\n",
        "    for i in lines:\n",
        "      line = re.sub(\"\\n.\\n',\",'.',i)\n",
        "      line = re.sub('\\n<s>.','',line)\n",
        "      line = re.sub('\\n<s>','',line)\n",
        "      line = re.sub('\\n',' ',line)\n",
        "      line = re.sub('<s>','',line)\n",
        "      line = line.lstrip()\n",
        "      line = line.rstrip()\n",
        "      data.append(line)\n",
        "  return data"
      ],
      "metadata": {
        "id": "UEmcMiwBVdO7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source ='/content/train-source.txt' \n",
        "target = '/content/train-target.txt'\n"
      ],
      "metadata": {
        "id": "TgTsK2O1WH_e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_data = array(preprocess(target))\n",
        "source_data = array(preprocess(source))"
      ],
      "metadata": {
        "id": "qzE1IMkTPQgG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_data.shape == target_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlev8ZR1Qvi4",
        "outputId": "80d5c30a-28a1-4e81-e64d-8690e6586886"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Heavily taken from: https://github.com/AarohiSingla/Language-translator-using-seq2seq-model/blob/main/seq2seq_full_code_algo.ipynb"
      ],
      "metadata": {
        "id": "m646vxMixV5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#source_data = source_data[:20000]\n",
        "#source_data.shape\n",
        "def tokenization(data):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(data)\n",
        "  return tokenizer\n",
        "\n",
        "source_tokenizer = tokenization(source_data)\n",
        "source_vocab_size = len(source_tokenizer.word_index)+1\n",
        "\n",
        "target_tokenizer = tokenization(target_data)\n",
        "target_vocab_size = len(target_tokenizer.word_index)+1"
      ],
      "metadata": {
        "id": "YhbvyntXQvlV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sequences(tokenizer, length, lines):\n",
        "\t# integer encode sequences\n",
        "\tX = tokenizer.texts_to_sequences(lines)\n",
        "\t# pad sequences with 0 values\n",
        "\tX = pad_sequences(X, maxlen=length, padding='post')\n",
        "\treturn X"
      ],
      "metadata": {
        "id": "mBxXB7nnQvnv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_output(sequences, vocab_size):\n",
        "\tylist = list()\n",
        "\tfor sequence in sequences:\n",
        "\t\tencoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "\t\tylist.append(encoded)\n",
        "\ty = array(ylist)\n",
        "\ty = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "\treturn y"
      ],
      "metadata": {
        "id": "AdGVKm2rv-E-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_characters = set()\n",
        "target_characters = set()\n",
        "for line in source_data:\n",
        "  for word in line:\n",
        "    for char in word:\n",
        "      if char not in input_characters:\n",
        "        input_characters.add(char)\n",
        "for line in target_data:\n",
        "  for word in line:\n",
        "    for char in word:\n",
        "      if char not in target_characters:\n",
        "        target_characters.add(char)"
      ],
      "metadata": {
        "id": "5u0kuiQhRAKA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = source_data\n",
        "target_texts = target_data"
      ],
      "metadata": {
        "id": "WJlqOyDzRWlw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ],
      "metadata": {
        "id": "K_bRjhRKPSge"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXG13sHHPSi6",
        "outputId": "3bfee9fc-0285-456d-9f39-fc8106017936"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 45172\n",
            "Number of unique input tokens: 107\n",
            "Number of unique output tokens: 94\n",
            "Max sequence length for inputs: 1190\n",
            "Max sequence length for outputs: 1113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])\n"
      ],
      "metadata": {
        "id": "c15A3sFiPSlD"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tokenizer(lines):\n",
        "\ttokenizer = Tokenizer()\n",
        "\ttokenizer.fit_on_texts(lines)\n",
        "\treturn tokenizer"
      ],
      "metadata": {
        "id": "Ty3VqDS2PSwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sor_tokenizer = create_tokenizer(source_data[:])\n",
        "sor_vocab_size = len(sor_tokenizer.word_index) + 1\n",
        "sor_length = max_encoder_seq_length(source_data[:])\n",
        "print('English Vocabulary Size: %d' % sor_vocab_size)\n",
        "print('English Max Length: %d' % (sor_length))\n",
        "\n",
        "tar_tokenizer = create_tokenizer(target_data[:])\n",
        "tar_vocab_size = len(tar_tokenizer.word_index) + 1\n",
        "tar_length = max_decoder_seq_length(target_data[:])\n",
        "print('English Vocabulary Size: %d' % tar_vocab_size)\n",
        "print('English Max Length: %d' % (tar_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufVlmTHCWVoJ",
        "outputId": "9d835e0e-2055-4f15-8aa7-3ea57fa77679"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 27497\n",
            "English Max Length: 231\n",
            "English Vocabulary Size: 24968\n",
            "English Max Length: 221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainX = encode_sequences(tar_tokenizer, tar_length,source_data)"
      ],
      "metadata": {
        "id": "S1w9hmVPexBu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainY = encode_sequences(sor_tokenizer, sor_length, source_data)"
      ],
      "metadata": {
        "id": "LIbOU1c3WVvO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainY = encode_output(trainY, sor_vocab_size)"
      ],
      "metadata": {
        "id": "qWiqsWcDePYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testX = encode_sequences(tar_tokenizer, tar_length, target_data)"
      ],
      "metadata": {
        "id": "scbrBOcwa5O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testY = encode_sequences(sor_tokenizer, sor_length, target_data)"
      ],
      "metadata": {
        "id": "gO9GoNcfe3lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testY = encode_output(testY, sor_vocab_size)"
      ],
      "metadata": {
        "id": "cu3lcmx9e3v6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "trainX,trainY,testX,testY = train_test_split(source_data,target_data,test_size=0.2)"
      ],
      "metadata": {
        "id": "maqQs0E6c4Nn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "\tmodel.add(LSTM(n_units))\n",
        "\tmodel.add(RepeatVector(tar_timesteps))\n",
        "\tmodel.add(LSTM(n_units, return_sequences=True))\n",
        "\tmodel.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
        "\treturn model"
      ],
      "metadata": {
        "id": "BPRpdmiue3zM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = define_model(sor_vocab_size, tar_vocab_size, sor_length, tar_length, 256)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy')"
      ],
      "metadata": {
        "id": "3Z_rzm9pa5SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY), verbose=2)"
      ],
      "metadata": {
        "id": "bzquPsFpa5r9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CBgqdVa5VQt5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}